# æŸ¥çœ‹æ•°æ®ç»“æ„
import pandas as pd
import warnings
warnings.filterwarnings('ignore')#å¿½ç•¥è­¦å‘Š
df=pd.read_csv("D:/ç¤ºä¾‹æ•°æ®é›†/titanicæ•°æ®é›†/train.csv")#å¯¼å…¥è®­ç»ƒé›†æ•°æ®
dft=pd.read_csv("D:/ç¤ºä¾‹æ•°æ®é›†/titanicæ•°æ®é›†/test.csv")#å¯¼å…¥æµ‹è¯•é›†æ•°æ®
df.head()#æŸ¥çœ‹è®­ç»ƒé›†æ•°æ®
dft.head()
df.info()#æŸ¥çœ‹æ•°æ®ç»“æ„
dft.info()

#è°ƒæ•´æ•°æ®åˆ—
def move_column_to_end(df, col_name):
    """å°†æŒ‡å®šåˆ—ç§»åŠ¨åˆ° DataFrame çš„æœ€åä¸€åˆ—"""
    if col_name not in df.columns:
        raise ValueError(f"åˆ— '{col_name}' ä¸å­˜åœ¨äº DataFrame ä¸­")
    cols = [col for col in df.columns if col != col_name] + [col_name]
    return df[cols]
df= move_column_to_end(df, 'Survived')
df.head()


#åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†è¿›è¡Œç¼ºå¤±å€¼å¤„ç†
dfi=pd.concat([df,dft])#åˆå¹¶æ•°æ®
dfi.info()
dfi.isna().sum()#æŸ¥çœ‹ç¼ºå¤±å€¼

#æŸ¥çœ‹æ•°æ®ç¼ºå¤±å€¼çš„åˆ†å¸ƒæƒ…å†µ
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
sns.heatmap(dfi.isnull(), cbar=True, yticklabels=False, cmap='viridis')
plt.title('Missing Value Heatmap')
plt.show()

#ç”»å›¾å±•ç¤ºæ•°æ®åˆ†å¸ƒæƒ…å†µ
import matplotlib.pyplot as plt
plt.rcParams["font.family"] = "SimHei"  # ä¸­æ–‡é»‘ä½“æ˜¾ç¤º
plt.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºå¼‚å¸¸çš„é—®é¢˜ï¼ˆå¯é€‰ï¼‰
#1Pclassä¸è¾“å‡ºå˜é‡ä¹‹é—´çš„å…³ç³»ï¼ˆPclassè¡¨ç¤ºèˆ¹èˆ±çš„ç­‰çº§ï¼‰
train=dfi.head(len(df))
train.info()
train.isna().sum()
#åˆ†ç±»å æ¯”
train.groupby('Pclass')['Survived'].mean()
# ç”»å›¾å±•ç¤º
fig=plt.figure()
fig.set(alpha=0.5)#è®¾å®šé¢œè‰²çš„å‚æ•°
Survived_0=train.Pclass[train.Survived==0].value_counts()#è®¡ç®—è®­ç»ƒé›†ä¸­æœªè·æ•‘çš„Pclassçš„ä¸ªæ•°
Survived_1=train.Pclass[train.Survived==1].value_counts()#è®¡ç®—è®­ç»ƒé›†ä¸­è·æ•‘çš„Pclassçš„ä¸ªæ•°
df_S=pd.DataFrame({"æœªè·æ•‘":Survived_0,"è·æ•‘":Survived_1})#æ„å»ºdataframeæ•°æ®ç»“æ„
df_S.plot(kind='bar',stacked=True)
plt.title("ä¹˜å®¢ç­‰çº§çš„è·æ•‘æƒ…å†µ")
plt.xlabel("ä¹˜å®¢ç­‰çº§")
plt.ylabel("æ•°é‡")
plt.show()

#ä¸åŒæ€§åˆ«çš„å­˜æ´»ç‡
train.groupby('Sex')['Survived'].mean()
#ä¸åŒèˆ¹èˆ±ç­‰çº§çš„å­˜æ´»ç‡
train.groupby('Pclass')['Survived'].mean()
#ä¸åŒç™»èˆ¹æ¸¯å£çš„å­˜æ´»ç‡
train.groupby('Embarked')['Survived'].mean()
train.groupby('Sex')['Survived'].mean()
#å¹´é¾„å­˜æ´»ç‡çš„æ­£æ€åˆ†å¸ƒå›¾
age_kd=sns.kdeplot(train["Age"][train["Survived"]==1],color='red',shade=True)
age_kd=sns.kdeplot(train['Age'][train['Survived']==0],color='grey',shade=True)
age_kd.set_xlabel('Age')
age_kd.set_ylabel('Density')
age_kd = age_kd.legend(['Survived', 'Not Survived'])

# ä½¿ç”¨pd.crosstabç”Ÿæˆæ€§åˆ«ä¸èˆ±ä½ç­‰çº§çš„äº¤å‰è¡¨
cross_tab = pd.crosstab(train['Sex'], train['Pclass'])
print(cross_tab)
#ç”»æ•£ç‚¹å›¾æŸ¥çœ‹ç”Ÿå­˜æƒ…å†µä¸è´¹ç”¨çš„å…³ç³»
plt.figure(figsize=(10, 6))
sns.swarmplot(
    data=train,
    x='Survived',
    y='Fare',
    size=3,
    alpha=0.7
)
plt.title('Fare vs Survival (Swarm Plot)')
plt.xlabel('Survived')
plt.ylabel('Fare')
plt.ylim(0, 200)
plt.show()


#æ„å»ºå®¶åº­æ•°é‡ä¸ç”Ÿå­˜ç‡çš„å…³ç³»
train['FamilySize'] = train['SibSp'] + train['Parch'] + 1  # åŒ…å«è‡ªå·±
survival_by_family = train.groupby('FamilySize')['Survived'].mean().reset_index()
survival_by_family.columns = ['FamilySize', 'SurvivalRate']
# åŒæ—¶ç»Ÿè®¡æ¯ç»„äººæ•°ï¼ˆç”¨äºè¯„ä¼°å¯é æ€§ï¼‰
count_by_family = train.groupby('FamilySize')['Survived'].count().reset_index()
count_by_family.columns = ['FamilySize', 'Count']
# åˆå¹¶ç”Ÿå­˜ç‡å’Œäººæ•°
result = pd.merge(survival_by_family, count_by_family, on='FamilySize')
print(result)
plt.figure(figsize=(10, 6))
ax = sns.barplot(data=result, x='FamilySize', y='SurvivalRate', palette='viridis')

# åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºç”Ÿå­˜ç‡ç™¾åˆ†æ¯”å’Œäººæ•°
for i, row in result.iterrows():
    ax.text(
        i, 
        row['SurvivalRate'] + 0.01, 
        f"{row['SurvivalRate']:.1%}\n(n={row['Count']})", 
        ha='center', 
        va='bottom',
        fontsize=9
    )
plt.title('Survival Rate by Family Size ')
plt.xlabel('Family Size')
plt.ylabel('Survival Rate')
plt.ylim(0, 1)
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

#æ€§åˆ«ä¸ç”Ÿå­˜ç‡ä¹‹é—´æœ‰æ— å…³ç³»
from scipy.stats import chi2_contingency
# æ­¥éª¤1ï¼šæ„å»ºåˆ—è”è¡¨ï¼ˆäº¤å‰è¡¨ï¼‰
contingency_table = pd.crosstab(train['Sex'], train['Survived'])
print("åˆ—è”è¡¨ï¼ˆSex vs Survivedï¼‰:")
print(contingency_table)
print()
# æ­¥éª¤2ï¼šæ‰§è¡Œå¡æ–¹æ£€éªŒ
chi2, p, dof, expected = chi2_contingency(contingency_table)
# æ­¥éª¤3ï¼šè¾“å‡ºç»“æœå¹¶è§£è¯»
print("å¡æ–¹æ£€éªŒç»“æœ:")
print(f"å¡æ–¹ç»Ÿè®¡é‡ (Ï‡Â²): {chi2:.4f}")
print(f"p å€¼: {p:.6f}")
print(f"è‡ªç”±åº¦ (dof): {dof}")
print("æœŸæœ›é¢‘æ•°ï¼ˆè‹¥ç‹¬ç«‹ï¼‰:")
print(expected)
# æ­¥éª¤4ï¼šå‡è®¾æ£€éªŒç»“è®º
alpha = 0.05
if p < alpha:
    print(f"\nç»“è®ºï¼šåœ¨æ˜¾è‘—æ€§æ°´å¹³ Î±={alpha} ä¸‹ï¼Œæ‹’ç»åŸå‡è®¾ã€‚")
    print("â†’ æ€§åˆ«ä¸ç”Ÿå­˜çŠ¶æ€ä¸ç‹¬ç«‹ï¼Œå­˜åœ¨æ˜¾è‘—å…³è”ã€‚")
else:
    print(f"\nç»“è®ºï¼šåœ¨æ˜¾è‘—æ€§æ°´å¹³ Î±={alpha} ä¸‹ï¼Œæ— æ³•æ‹’ç»åŸå‡è®¾ã€‚")
    print("â†’ æ²¡æœ‰è¶³å¤Ÿè¯æ®è¡¨æ˜æ€§åˆ«ä¸ç”Ÿå­˜çŠ¶æ€ç›¸å…³ã€‚")



#è®¡ç®—æ•°å€¼ç‰¹å¾ä¸ Survived çš„ Spearman ç›¸å…³ç³»æ•°
import pandas as pd
import numpy as np
from scipy.stats import spearmanr
df_corr = train.copy()
# å°† Sex ç¼–ç ä¸º 0/1ï¼ˆmale=0, female=1ï¼‰
df_corr['Sex'] = df_corr['Sex'].map({'male': 0, 'female': 1})
# ç¡®ä¿ Survived æ˜¯æ•°å€¼å‹ï¼ˆé€šå¸¸æ˜¯ intï¼‰
df_corr['Survived'] = df_corr['Survived'].astype(int)
# é€‰æ‹©è¦è®¡ç®—ç›¸å…³æ€§çš„æ•°å€¼åˆ—
numeric_cols = ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex']
# è®¡ç®— Spearman ç›¸å…³ç³»æ•°ï¼ˆåªå…³å¿ƒä¸ Survived çš„ç›¸å…³æ€§ï¼‰
corr_results = []
for col in numeric_cols:
    if col == 'Survived':
        continue  # è·³è¿‡è‡ªèº«
    # åˆ é™¤è¯¥åˆ—å’Œ Survived ä¸­ä»»ä¸€ä¸º NaN çš„è¡Œ
    valid_data = df_corr[['Survived', col]].dropna()
    if len(valid_data) < 2:
        corr, pval = np.nan, np.nan
    else:
        corr, pval = spearmanr(valid_data['Survived'], valid_data[col])
    corr_results.append({
        'Feature': col,
        'Spearman_Corr': corr,
        'p_value': pval
    })
# è½¬ä¸º DataFrame å¹¶æ’åºï¼ˆæŒ‰ç»å¯¹ç›¸å…³æ€§é™åºï¼‰
corr_df = pd.DataFrame(corr_results)
corr_df['Abs_Corr'] = corr_df['Spearman_Corr'].abs()
corr_df = corr_df.sort_values('Abs_Corr', ascending=False).reset_index(drop=True)
print("å„ç‰¹å¾ä¸ Survived çš„ Spearman ç›¸å…³ç³»æ•°ï¼š")
print(corr_df[['Feature', 'Spearman_Corr', 'p_value']].round(4))


#ç¼ºå¤±å€¼å¤„ç†ï¼Œä½¿ç”¨å‡å€¼å¡«è¡¥ç¼ºå¤±å€¼
dfi["Age"]=dfi["Age"].fillna(dfi["Age"].mean())
dfi["Fare"].value_counts()#æŸ¥çœ‹ç¼ºå¤±å€¼æƒ…å†µ
dfi["Fare"]=dfi["Fare"].fillna(dfi["Fare"].mean())
dfi["Cabin"].value_counts()
dfi["Cabin"]=dfi["Cabin"].fillna("Unkonw")
dfi['Deck']=dfi['Cabin'].map(lambda x:x[0])#æå–é¦–å­—æ¯ä½œä¸ºä¸€ä¸ªæ–°çš„åˆ—
dfi["Cabin"].value_counts()
dfi.isna().sum()#æŸ¥çœ‹ç¼ºå¤±å€¼
dfi['FamilySize'] = dfi['SibSp'] + dfi['Parch'] + 1  # åŒ…å«è‡ªå·±
dfi.head()

#åˆ†ç±»æ•°æ®è½¬æ¢
#Nameä¸­æå–ç§°è°“title
#å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è·å–å§“åä¿¡æ¯
def reclassis(name):
    s1=name.split(",")[1]
    s2=s1.split(".")[0]
    return s2.strip()
dfi['title']=dfi['Name'].map(reclassis)
dfi['title'].value_counts()#åˆ†ç±»è®¡æ•°
dfi_dict={'Miss': 'Miss',
    'Mlle': 'Miss',  
    'Ms': 'Miss',   
    'Mme': 'Mrs',        
    'Mrs': 'Mrs',
    'Mr': 'Mr',
    'Master': 'Master', 
    # ä¸“ä¸š/è´µæ—å¤´è¡” â†’ å½’ä¸º Rare
    'Dr': 'Rare',
    'Rev': 'Rare',       # ç‰§å¸ˆ
    'Col': 'Rare',       # ä¸Šæ ¡
    'Major': 'Rare',
    'Capt': 'Rare',      # èˆ¹é•¿
    'Sir': 'Rare',       # çˆµå£«
    'Don': 'Rare',       # è¥¿ç­ç‰™è´µæ—
    'Jonkheer': 'Rare',  # è·å…°è´µæ—
    'Countess': 'Rare',  # å¥³ä¼¯çˆµ
    'Lady': 'Rare',      # è´µå¦‡
    'Dona': 'Rare'       # è‘¡è„ç‰™/è¥¿ç­ç‰™è´µå¦‡
         }
dfi['title']=dfi['title'].map(dfi_dict).fillna('Rare')
dfi['title'].value_counts()#åˆ†ç±»
dfi.head(891).groupby('title')['Survived'].mean()#æŸ¥çœ‹ä¸åŒç±»åˆ«çš„äººçš„å­˜æ´»ç‡
#åˆ¤æ–­æ˜¯å¦å•ç‹¬
def IsAlone(x):
    if x==1:
        return 'alone'
    else:
        return 'more'
dfi['alone']=dfi['FamilySize'].map(IsAlone)
dfi.head()

#è§„å®šå¹´é¾„
def Olderflip(x):
    if x<=16:
        return 'child'
    elif 16<x<=35:
        return "young"
    elif 35<x<=60:
        return "old"
    else:
        return 'older'
dfi['Olderf']=dfi['Age'].map(Olderflip)
dfi.head()
dfi.head(891).groupby('Olderf')['Survived'].mean()
dfi.describe()
def FareBand(x):
    if x<=7.895800:
        return 'ä½'
    elif 7.895800<x<=14.454200:
        return "åä½"
    elif 14.454200<x<=31.275000:
        return "åé«˜"
    else:
        return 'é«˜'
dfi['FB']=dfi['Fare'].map(FareBand)
dfi.head()
dfi.head(891).groupby('FB')['Survived'].mean()
dfi.head()
#å°†åˆ†ç±»å˜é‡è½¬æ¢æˆæ•°å­—ï¼ˆç‹¬çƒ­ç¼–ç ï¼‰ï¼Œå¯¹æ‰€æœ‰çš„åˆ†ç±»å˜é‡è¿›è¡Œæ­¤æ“ä½œï¼Œé¿å…æ¨¡å‹è®­ç»ƒäº§ç”Ÿé—®é¢˜,å¹¶å¯¹è¿ç»­å˜é‡è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
dummies_Embarked=pd.get_dummies(dfi['Embarked'])
D_Sex=pd.get_dummies(dfi['Sex'])
D_Deck=pd.get_dummies(dfi['Deck'])
D_Pclass=pd.get_dummies(dfi['Pclass'])
D_title=pd.get_dummies(dfi['title'])
D_alone=pd.get_dummies(dfi['alone'])
D_Olderf=pd.get_dummies(dfi['Olderf'])
D_FB=pd.get_dummies(dfi['FB'])

#æ•°æ®æ ‡å‡†åŒ–
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
dfi["age_scaled"]=scaler.fit_transform(dfi["Age"].values.reshape(-1,1))
dfi["fare_scaled"]=scaler.fit_transform(dfi["Fare"].values.reshape(-1,1))
dff=pd.concat([dfi,dummies_Embarked,D_Sex,D_Deck,D_Pclass,D_title,D_alone,D_Olderf,D_FB],axis=1)
dff.info()
# åˆ é™¤ä¸å¿…è¦çš„åˆ—æ¬¡
dropcols=['Embarked','title','PassengerId','Pclass','Name','Sex','SibSp','Age','Fare','Parch','Ticket','Cabin','Deck','FB','FamilySize','alone','Olderf']
dff.drop(columns=dropcols, inplace=True)
dff.head(2)
dff.info()



# æ¨¡å‹è®­ç»ƒ
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')
# === æ•°æ®å‡†å¤‡ ===
trainf = dff.head(891)
x = trainf.drop("Survived", axis=1)
# å…³é”®ä¿®å¤ï¼šå°† Survived åˆ—è½¬æ¢ä¸ºæ•´æ•°ç±»å‹
y = trainf["Survived"].astype(int)  # æ·»åŠ  .astype(int)
print("Survived åˆ—çš„å€¼åˆ†å¸ƒ:")
print(y.value_counts())
print(f"Survived æ•°æ®ç±»å‹: {y.dtype}")
# === åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† ===
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42, stratify=y
)
# === æ¨¡å‹è¯„ä¼° ===
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision', 
    'recall': 'recall',
    'f1': 'f1',
    'roc_auc': 'roc_auc'
}
def evaluate_models(x, y, cv=5):
    # å°†æ•°æ®è½¬æ¢ä¸º NumPy æ•°ç»„ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜
    x_array = x.values if hasattr(x, 'values') else x
    y_array = y.values if hasattr(y, 'values') else y
    
    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Random Forest': RandomForestClassifier(random_state=42),
        'SVM': SVC(probability=True, random_state=42),
        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
        'KNN': KNeighborsClassifier()
    }
    results = {}
    
    for name, model in models.items():
        print(f"Running CV for {name}...")
        try:
            cv_results = cross_validate(model, x_array, y_array, cv=cv, scoring=scoring, n_jobs=-1)
            results[name] = {
                'Accuracy': cv_results['test_accuracy'].mean(),
                'Precision': cv_results['test_precision'].mean(),
                'Recall': cv_results['test_recall'].mean(),
                'F1': cv_results['test_f1'].mean(),
                'AUC': cv_results['test_roc_auc'].mean()
            }
            print(f"  {name} å®Œæˆ: Accuracy = {results[name]['Accuracy']:.4f}")
        except Exception as e:
            print(f"  {name} å‡ºé”™: {e}")
            # è·³è¿‡å‡ºé”™çš„æ¨¡å‹
            continue
    
    return pd.DataFrame(results).T if results else pd.DataFrame()

# æ‰§è¡Œäº¤å‰éªŒè¯
cv_results_df = evaluate_models(x_train, y_train, cv=5)

if not cv_results_df.empty:
    print("\n=== 5-Fold Cross Validation Results ===")
    print(cv_results_df.round(4))
    
    # æµ‹è¯•é›†è¯„ä¼°
    print("\n=== æµ‹è¯•é›†è¯„ä¼° ===")
    best_model_name = cv_results_df['Accuracy'].idxmax()
    print(f"æœ€ä½³æ¨¡å‹: {best_model_name}")
    
    # é‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹
    model_mapping = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Random Forest': RandomForestClassifier(random_state=42),
        'SVM': SVC(probability=True, random_state=42),
        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
        'KNN': KNeighborsClassifier()
    }
    
    best_model = model_mapping[best_model_name]
    best_model.fit(x_train.values, y_train.values)  # ä½¿ç”¨ .values ç¡®ä¿å…¼å®¹æ€§
    
    y_pred = best_model.predict(x_test.values)
    test_accuracy = accuracy_score(y_test, y_pred)
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
else:
    print("æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥æ•°æ®")


# åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.preprocessing import LabelEncoder
import numpy as np
# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ˜¾ç¤º
sns.set_style("whitegrid")#è®¾ç½®å›¾ç‰‡ä¸ºç™½åº•åŠ ç½‘æ ¼çš„å½¢å¼
# === é€‰æ‹©æœ€ä¼˜æ¨¡å‹ ===
print("=== æ¨¡å‹æ€§èƒ½æ€»ç»“ ===")
print(cv_results_df.round(4))
# æ ¹æ®AUCé€‰æ‹©æœ€ä¼˜æ¨¡å‹ï¼ˆAUCå¯¹ä¸å¹³è¡¡æ•°æ®æ›´é²æ£’ï¼‰
best_model_name = cv_results_df['AUC'].idxmax()
print(f"\nğŸ¯ æœ€ä¼˜æ¨¡å‹é€‰æ‹©: {best_model_name}")
print(f"æœ€ä½³AUC: {cv_results_df.loc[best_model_name, 'AUC']:.4f}")
print(f"æœ€ä½³å‡†ç¡®ç‡: {cv_results_df.loc[best_model_name, 'Accuracy']:.4f}")
# === é‡æ–°è®­ç»ƒæœ€ä¼˜æ¨¡å‹ ===
model_mapping = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(probability=True, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    'KNN': KNeighborsClassifier()
}
best_model = model_mapping[best_model_name]
print(f"\n=== è®­ç»ƒæœ€ä¼˜æ¨¡å‹ ({best_model_name}) ===")
print(best_model.fit(x_train.values, y_train.values))
# === åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹ ===
y_pred = best_model.predict(x_test.values)
y_pred_proba = best_model.predict_proba(x_test.values)[:, 1]
print(y_pred)
print(y_pred_proba)
# è‡ªåŠ¨é€‰æ‹©å¯ç”¨ä¸­æ–‡å­—ä½“
def set_chinese_font():
    import platform
    system = platform.system()
    if system == "Windows":
        plt.rcParams['font.sans-serif'] = ['SimHei']
    elif system == "Darwin":  # macOS
        plt.rcParams['font.sans-serif'] = ['PingFang HK', 'Arial Unicode MS', 'Hiragino Sans GB']
    else:  # Linux
        plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
set_chinese_font()
# === 1. æ··æ·†çŸ©é˜µ ===
print("\n=== æ··æ·†çŸ©é˜µ ===")
cm = confusion_matrix(y_test, y_pred)
print(f"æ··æ·†çŸ©é˜µ:\n{cm}")


# ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
plt.figure(figsize=(15, 4))
plt.subplot(1, 3, 1)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['æ­»äº¡(0)', 'ç”Ÿå­˜(1)'], 
            yticklabels=['æ­»äº¡(0)', 'ç”Ÿå­˜(1)'])
plt.title(f'{best_model_name} - æ··æ·†çŸ©é˜µ')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.ylabel('çœŸå®æ ‡ç­¾')
# === 2. åˆ†ç±»æŠ¥å‘Š ===
print("\n=== è¯¦ç»†åˆ†ç±»æŠ¥å‘Š ===")
report = classification_report(y_test, y_pred, target_names=['æ­»äº¡(0)', 'ç”Ÿå­˜(1)'])
print(report)

# === 3. ROCæ›²çº¿å’ŒAUCå€¼ ===
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
print(f"\n=== ROCæ›²çº¿åˆ†æ ===")
print(f"AUCå€¼: {roc_auc:.4f}")
# ç»˜åˆ¶ROCæ›²çº¿
plt.subplot(1, 3, 2)
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='éšæœºçŒœæµ‹')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('å‡æ­£ç‡ (False Positive Rate)')
plt.ylabel('çœŸæ­£ç‡ (True Positive Rate)')
plt.title(f'{best_model_name} - ROCæ›²çº¿')
plt.legend(loc="lower right")
plt.grid(True)
# === 4. æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾ ===
plt.subplot(1, 1, 1)
metrics = ['Accuracy', 'Precision', 'Recall', 'F1']
models = cv_results_df.index
x_pos = np.arange(len(models))
# åˆ›å»ºåˆ†ç»„æŸ±çŠ¶å›¾
bar_width = 0.2
for i, metric in enumerate(metrics):
    values = cv_results_df[metric].values
    plt.bar(x_pos + i * bar_width, values, bar_width, 
            label=metric, alpha=0.8)
plt.xlabel('æœºå™¨å­¦ä¹ æ¨¡å‹')
plt.ylabel('åˆ†æ•°')
plt.title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
plt.xticks(x_pos + bar_width * 1.5, models, rotation=45)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()
# === 6. æ¨¡å‹æ¯”è¾ƒæ€»ç»“ ===
print("\n=== æ‰€æœ‰æ¨¡å‹æ€§èƒ½æ’å ===")
for metric in ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']:
    best_model_for_metric = cv_results_df[metric].idxmax()
    best_score = cv_results_df[metric].max()
    print(f"{metric}: {best_model_for_metric} ({best_score:.4f})")
# === 7. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ ===
plt.figure(figsize=(12, 5))
plt.subplot(1, 1, 1)
# ç”Ÿå­˜æ¦‚ç‡åˆ†å¸ƒ
plt.hist(y_pred_proba[y_test == 1], alpha=0.7, label='å®é™…ç”Ÿå­˜', bins=20)
plt.hist(y_pred_proba[y_test == 0], alpha=0.7, label='å®é™…æ­»äº¡', bins=20)
plt.xlabel('é¢„æµ‹ç”Ÿå­˜æ¦‚ç‡')
plt.ylabel('é¢‘æ•°')
plt.title('é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ')
plt.legend()
plt.tight_layout()
plt.show()
print(f"\næ¨¡å‹è¯„ä¼°å®Œæˆï¼æœ€ä¼˜æ¨¡å‹ {best_model_name} åœ¨éªŒè¯é›†ä¸Šè¡¨ç°è‰¯å¥½ã€‚")


# è¾“å‡ºç»“æœ
import pandas as pd
from sklearn.preprocessing import StandardScaler
# åŠ è½½æ–°æ•°æ®
new_data1 = dff.iloc[-418:]          # æœ€å418è¡Œ
# new_data2 = new_data1.loc[:,:-1]      
dropcols = ['Survived']
# åˆ é™¤ç›®æ ‡åˆ—
new_data1.drop(columns=dropcols, inplace=True, errors='ignore')
# å…³é”®ä¿®å¤ï¼šç»Ÿä¸€åˆ—åä¸ºå­—ç¬¦ä¸²
new_data1.columns = new_data1.columns.astype(str)
# æ£€æŸ¥ç¼ºå¤±å€¼ï¼ˆå¯é€‰ï¼‰
print(new_data1.isna().sum())
# é¢„æµ‹
predictions = best_model.predict(new_data1)
predictions_proba = best_model.predict_proba(new_data1)[:, 1]
# æ·»åŠ ç»“æœåˆ°åŸå§‹æ•°æ®
new_data_with_predictions = dft.copy()
new_data_with_predictions['Predicted_Survived'] = predictions
new_data_with_predictions['Predicted_Probability'] = predictions_proba
# å¯¼å‡º
output_file_path = "D:/ç¤ºä¾‹æ•°æ®é›†/titanicæ•°æ®é›†/ptest.xlsx"  # æ”¹ä¸º .xlsx
new_data_with_predictions.to_excel(output_file_path, index=False)
print(f"é¢„æµ‹ç»“æœå·²æˆåŠŸä¿å­˜è‡³ {output_file_path}")